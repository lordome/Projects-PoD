{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import initializers, regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from numpy import fft\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPGheArt - Competition\n",
    "\n",
    "### Lorenzo Buriola - 2021860\n",
    "### Filippo Conforto - 2021856\n",
    "### Lorenzo Domenichetti - 2011653\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(\"DATA/x_ts_comp_N10000.csv\", delimiter =\",\")\n",
    "categ_y = np.loadtxt(\"DATA/y_ts_comp_N10000.csv\", delimiter =\",\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x =  scaler.fit_transform(x.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(categ_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(x[0])\n",
    "input_shape = (len(x[0]),1)\n",
    "N_categ = 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = x.reshape(x.shape[0],L,1) #Reshaping x_data to use the Conv1D layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation obtained by flipping the time series\n",
    "\n",
    "temp = np.flip(x, axis = 1)\n",
    "\n",
    "x2 = np.concatenate([x,temp], axis = 0)\n",
    "y2 = np.concatenate([y,y], axis = 0)\n",
    "\n",
    "L = len(x2[0])\n",
    "input_shape = (len(x2[0]),1)\n",
    "N_categ = 3\n",
    "\n",
    "x2 = x2.reshape(x2.shape[0],L,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = initializers.RandomNormal(mean = 0, stddev = 0.05)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters = 10, kernel_size = 21,\n",
    "                    kernel_regularizer = regularizers.l2(0.01), #L2 regularizer with intermediate value of lambda\n",
    "                    kernel_initializer=ini,\n",
    "                    padding = \"same\", #Data padding\n",
    "                    activation = \"relu\",\n",
    "                    input_shape = input_shape\n",
    "                    ))\n",
    "\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3,activation=\"relu\")) #Small dense layer \n",
    "\n",
    "model.add(Dense(N_categ, activation=\"softmax\"))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_22 (Conv1D)           (None, 60, 10)            220       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 363       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 595\n",
      "Trainable params: 595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.save_weights(\"competition_weights.h5\") #Da togliere\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "acc = []\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "fig, axes = plt.subplots(1,5, figsize = (10,20))\n",
    "for i, (train, test) in enumerate(kfold.split(x2,y2)):\n",
    "    model.load_weights(\"competition_weights.h5\")\n",
    "    hist = model.fit(x2[train], y2[train], batch_size = 50, epochs = 150, \n",
    "                 #validation_data = (x_test, y_test), \n",
    "                verbose = 0, shuffle = True, use_multiprocessing=True,workers=16)\n",
    "    scores = model.evaluate(x2[test], y2[test], verbose=0)\n",
    "    plot_conf_mat(confusion_matrix(np.argmax(y2[test], axis =1),np.argmax(model.predict(x2[test]), axis=1),  normalize = 'true'), axes[i], title = '')\n",
    "    acc.append(scores[1])\n",
    "    print(scores[1])\n",
    "plt.show()\n",
    "print(np.asarray(acc).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x2, y2, batch_size = 250, epochs = 150, \n",
    "                verbose = 0, shuffle = True, use_multiprocessing=True,workers=16)\n",
    "model.save_weights(\"FPGheArt_parameters.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"FPGheArt_parameters.h5\")\n",
    "\n",
    "\n",
    "x_test = np.loadtxt(\"DATA/x_test_N10000.csv\", delimiter =\",\") #Test set import\n",
    "\n",
    "x_test =  scaler.fit_transform(x.T).T                         #Test set scaling\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],L,1)                  #Test set reshaping\n",
    "\n",
    "np.savetxt(\"FPGheArt_yhat.h5\", np.argmax(model.predict(x_test), axis=1)) #Save prediction results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
