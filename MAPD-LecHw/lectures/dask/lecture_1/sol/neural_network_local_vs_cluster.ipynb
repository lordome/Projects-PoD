{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to work with Cross Validation, Neural Networks and Clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple binary classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "print(\"Example of datset row: \"+str(X[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the ```Neural Network```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network():\n",
    "    \n",
    "    # create model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100, 200, 100), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the cross validation data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = []\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "for train, test in kfold.split(X, y):\n",
    "    data_split.append((train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our ```classifier``` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = build_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network classifier for each dataset train split and test the trained model on each test dataset split, and in the end we calculate the mean of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results = []\n",
    "for train_idx, test_idx in data_split:\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "    Y_pred = clf.predict(X[test_idx])\n",
    "    results.append(accuracy_score(y[test_idx], Y_pred))\n",
    "end_time = time.time()    \n",
    "print(\"Mean of the results: \"+str(np.array(results).mean())+\" in: \"+str(end_time-start_time)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process have took several seconds, but wath change if we use a cluster instead? Let's see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let create the cluster and share the dataset to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Client()\n",
    "client.scatter(X)\n",
    "client.scatter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define the process that will be distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_cross_validation(args):\n",
    "    train_idx, test_idx = args\n",
    "    clf = build_neural_network()\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "\n",
    "    y_pred = clf.predict(X[test_idx])\n",
    "    print(str(accuracy_score(y[test_idx], y_pred)))\n",
    "    return accuracy_score(y[test_idx], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the process and retrieve the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "futures = client.map(distribute_cross_validation, [(train_idx, test_idx) for train_idx, test_idx in data_split])\n",
    "results = client.gather(futures)\n",
    "\n",
    "end_time = time.time()   \n",
    "print(\"Mean of the results: \"+str(np.array(results).mean())+\" in: \"+str(end_time-start_time)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process took ~1/5 of the total time. In general approaches when you have to validate a reliable machine learning model, this process should be reapeated a huge number of times with random splits of data. Using cluster machines is essential to work in a proper way. Nowadays, the improvements of the deep models and the increase of the available data (big data) has further increase those necessities.\n",
    "\n",
    "Note: This is a simple approach, but clusters are extremely useful even when the ml algorithms can be parllelized. Apache Spark, that we will see in last lecture,  exploit those ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
